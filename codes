#import necessary libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import xgboost as xgb
from sklearn.cluster import KMeans
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import confusion_matrix, accuracy_score, classification_report
from sklearn.preprocessing import LabelEncoder
from imblearn.over_sampling import RandomOverSampler
from imblearn.under_sampling import RandomUnderSampler

#load dataset
data=pd.read_csv('digital_marketing_campaign_dataset.csv')
df=pd.read_csv('digital_marketing_campaign_dataset.csv')

#overview of dataset
print(df.head())
print(df.info())
print(df.describe())

#check for missing values 
print(df.isnull().sum())

#check for unique values in categorical columns
for column in df.columns:
  if df[column].dtype == 'object':
    print(f"\nUnique values in '{column}': {df[column].unique()}")

#drop irrelevant columns
df = df.drop(['CustomerID','AdvertisingPlatform','AdvertisingTool','ConversionRate'], axis=1)
data = data.drop(['CustomerID','AdvertisingPlatform','AdvertisingTool'], axis=1)

#display the updated data
print(df.head())
print(data.head())

!pip install wolta

from wolta.data_tools import unique_amounts
unique_amounts(df)

#####VISUALIZATION
#target variable
conversion_counts = data['Conversion'].value_counts()
labels = ['No Conversion', 'Conversion']
colors = ['lightblue', 'yellow']  #custom colours
plt.figure(figsize=(6, 6))
plt.pie(conversion_counts, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)
plt.title('Conversion Distribution')
plt.axis('equal') 
plt.show()

#age
age_bins = [18, 25, 35, 45, 55, 65, np.inf]
age_labels = ['18-24', '25-34', '35-44', '45-54', '55-64', '65+'] #create age ranges
colors = ['lightblue', 'lightgreen', 'lightcoral', 'lightsalmon', 'lightgrey', 'yellow']
df['Age_Group'] = pd.cut(df['Age'], bins=age_bins, labels=age_labels, right=False) #create new age group column 

#gender and age
fig, axes = plt.subplots(1, 2, figsize=(12, 6))
colors_gender = ['skyblue', 'lightpink']
gender_counts = df['Gender'].value_counts()
axes[0].pie(gender_counts, labels=gender_counts.index, autopct='%1.1f%%', startangle=90, colors=colors_gender)
axes[0].set_title('Distribution of Gender')
colors_age_group = ['lightgreen', 'lightpink', 'lightsalmon',
                    'lightblue', 'lightgrey', 'yellow']
age_group_counts = df['Age_Group'].value_counts()
axes[1].pie(age_group_counts, labels=age_group_counts.index, autopct='%1.1f%%', startangle=90, colors=colors_age_group)
axes[1].set_title('Distribution of Age Group')
plt.show()

#convert age group to categorical variable
df['Age_Group'] = pd.Categorical(df['Age_Group'], categories=age_labels, ordered=True)

gender_colors = {'Male': 'skyblue', 'Female': 'lightpink'}  #custom colour 
g = sns.FacetGrid(df, col="Gender", hue="Gender", palette=gender_colors)
g.map(plt.hist, "Age_Group", alpha=0.7)
for ax in g.axes.flat:
    ax.set_xticks(ax.get_xticks())  # Get current tick positions
    ax.set_xticklabels(age_labels)  # Set custom labels
    ax.tick_params(axis='x', rotation=45)  # Rotate labels if needed
plt.show()

#income
income_bins = [0, 25000, 50000, 75000, 100000, 125000, 150000, np.inf]
income_labels = ['0-25k', '25k-50k', '50k-75k', '75k-100k', '100k-125k', '125k-150k','150k+'] #create income ranges
df['Income_Group'] = pd.cut(df['Income'], bins=income_bins, labels=income_labels, include_lowest=True, right=False) #add new column for income bins
custom_palette = {'0-25k':'lightcoral','25k-50k':'lightgreen',
                  '50k-75k':'lightblue','75k-100k':'lightsalmon',
                  '100k-125k':'yellow', '125k-150k':'lightpink','150k+':'lightgrey'}
sns.countplot(x='Income_Group', data=df, palette=custom_palette)
plt.title('Income Group Distribution')
plt.xlabel('Income Group')
plt.ylabel('Count')
plt.show()

#gender-wise income
gender_income = df.groupby('Gender')['Income'].mean()
colors = {'Male': 'skyblue', 'Female': 'lightpink'} #custom colours
plt.bar(gender_income.index, gender_income.values, color=[colors[gender] for gender in gender_income.index])
plt.xlabel('Gender')
plt.ylabel('Average Income')
plt.title('Average Income by Gender')

#Average spend by campaign type
avg_spend = data.groupby(['CampaignChannel', 'CampaignType'])['AdSpend'].mean().reset_index()
custom_palette = {'Conversion': 'lightblue',
                  'Awareness': 'yellow',
                  'Retention': 'lightgreen',
                  'Consideration':'lightpink'} #custom colours
plt.figure(figsize=(12, 6))
sns.barplot(x='CampaignChannel', y='AdSpend', hue='CampaignType', data=avg_spend, palette=custom_palette)
plt.xlabel('Campaign Channel', fontsize=12)
plt.ylabel('Average Ad Spend', fontsize=12)
plt.title('Average Ad Spend per Campaign Channel and Type', fontsize=14)
plt.xticks(rotation=45, ha='right', fontsize=10)
plt.legend(title='Campaign Type', fontsize=10)
plt.tight_layout()
plt.show()

#age-wise average conversion rate
age_conversion = data.groupby('Age_Group')['ConversionRate'].mean()
colors = ['lightblue', 'lightgreen', 'lightcoral', 'lightsalmon', 'lightgrey', 'yellow']  #custom colours
plt.figure(figsize=(10, 6))
plt.barh(age_conversion.index, age_conversion.values, color=colors) 
plt.title('Conversion Rate by Age Group')
plt.ylabel('Age Group')
plt.xlabel('Conversion Rate')
plt.show()

#creating subplots
fig, axes = plt.subplots(3, 2, figsize=(15, 18))

# Boxplot for AdSpend vs Conversion
sns.boxplot(x='Conversion', y='AdSpend', data=data, hue='Conversion', palette={0: 'lightblue', 1: 'yellow'},ax=axes[0, 0])
axes[0, 0].set_title('AdSpend vs Conversion')

# Boxplot for WebsiteVisits vs Conversion
sns.boxplot(x='Conversion', y='WebsiteVisits', data=data, hue='Conversion', palette={0: 'lightblue', 1: 'yellow'},ax=axes[0, 1])
axes[0, 1].set_title('Website Visits vs Conversion')

# Boxplot for TimeOnSite vs Conversion
sns.boxplot(x='Conversion', y='TimeOnSite', data=data, hue='Conversion',palette={0: 'lightblue', 1: 'yellow'}, ax=axes[1, 0])
axes[1, 0].set_title('Time on Site vs Conversion')

# Boxplot for PreviousPurchases vs Conversion
sns.boxplot(x='Conversion', y='PreviousPurchases', data=data, hue='Conversion',palette={0: 'lightblue', 1: 'yellow'}, ax=axes[1, 1])
axes[1, 1].set_title('Previous Purchases vs Conversion')

# Boxplot for LoyaltyPoints vs Conversion
sns.boxplot(x='Conversion', y='LoyaltyPoints', data=data, hue='Conversion',palette={0: 'lightblue', 1: 'yellow'}, ax=axes[2, 0])
axes[2, 0].set_title('Loyalty Points vs Conversion')

# Remove the empty subplot (axes[2, 1])
fig.delaxes(axes[2, 1])
plt.tight_layout()

# Displaying the subplots
plt.show()

#number of previous purchases
purchase_counts = data['PreviousPurchases'].value_counts().sort_index()
plt.figure(figsize=(10, 6))
plt.bar(purchase_counts.index, purchase_counts.values)
plt.title('Frequency of Previous Purchase Counts')
plt.xlabel('Number of Previous Purchases')
plt.ylabel('Number of Customers')
plt.xticks(purchase_counts.index)  
plt.show()

#average conversion rate by campaign type
campaign_channel_conversion = data.groupby('CampaignChannel')['ConversionRate'].mean().reset_index()
plt.figure(figsize=(8, 6))
sns.barplot(x='CampaignChannel', y='ConversionRate', data=campaign_channel_conversion)
plt.xlabel('Campaign Channel')
plt.ylabel('Average Conversion Rate')
plt.title('Conversion Rate by Campaign Channel')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

#convert "Conversion" to categorical 
columns_to_convert = ['Conversion'] 
for column in columns_to_convert:
    data['Conversion'] = data['Conversion'].astype('category')
print(data.dtypes)

#correlation matrix and heatmap visualization
import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib.colors as mcolors

colors = ['pink', 'lightyellow', 'lightblue']  #custom colour
cmap = mcolors.LinearSegmentedColormap.from_list("mycmap", colors)

correlation_matrix = numerical_features.corr() #select only numerical features

# PLot heatmap
plt.figure(figsize=(12, 10))
sns.heatmap(correlation_matrix, annot=True, cmap=cmap, fmt=".2f")
plt.xticks(rotation=45, ha='right') 
plt.yticks(rotation=45, ha='right')  
plt.show()  
  
#Correlation for categorical variables 
!pip install scipy

from scipy.stats import chi2_contingency

def cramers_v(confusion_matrix):
    """Calculates Cramér's V statistic for a contingency table.

    Args:
        confusion_matrix: numpy.ndarray
            The contingency table/confusion matrix.

    Returns:
        float: Cramér's V statistic.
    """
    chi2 = chi2_contingency(confusion_matrix)[0]
    n = confusion_matrix.sum()
    phi2 = chi2 / n
    r, k = confusion_matrix.shape

    phi2corr = max(0, phi2 - ((k - 1) * (r - 1)) / (n - 1))
    rcorr = r - ((r - 1)**2) / (n - 1)
    kcorr = k - ((k - 1)**2) / (n - 1)
    return np.sqrt(phi2corr / min((kcorr - 1), (rcorr - 1)))

categorical_cols = ['Gender','CampaignChannel', 'CampaignType']  #select categorical columns

# Calculate Cramér's V for each pair of categorical columns
cramers_v_matrix = pd.DataFrame(index=categorical_cols, columns=categorical_cols)

for col1 in categorical_cols:
    for col2 in categorical_cols:
        confusion_matrix = pd.crosstab(df[col1], df[col2]).values
        cramers_v_matrix.loc[col1, col2] = cramers_v(confusion_matrix)

print(cramers_v_matrix)
#plot heatmap
import matplotlib.colors as mcolors
plt.figure(figsize=(10, 8))  
colors = ['pink', 'lightyellow', 'lightblue']  #custom colours
cmap = mcolors.LinearSegmentedColormap.from_list("mycmap", colors)
sns.heatmap(cramers_v_matrix.astype(float), annot=True, cmap=cmap, fmt=".2f", linewidths=.5)
plt.title("Cramér's V Correlation Matrix for Categorical Variables")
plt.xticks(rotation=45, ha='right')
plt.yticks(rotation=45, ha='right')
plt.show()

#Scale numerical variables
from sklearn.preprocessing import StandardScaler
numerical_features = df.select_dtypes(include=['number']).columns #select numerical variables
scaler = StandardScaler()
df[numerical_features] = scaler.fit_transform(df[numerical_features])
print(df.head())

#Encode categorical variables
from sklearn.preprocessing import LabelEncoder
columns_to_encode = ['Gender','CampaignChannel','CampaignType'] #select the categorical columns
label_encoder = LabelEncoder()
for column in columns_to_encode:
    df[column] = label_encoder.fit_transform(df[column])
print(df.head())

###MODELSSSS
from sklearn.model_selection import train_test_split
X = data.drop('Conversion', axis=1)
y = data['Conversion']

#Oversampling
from imblearn.over_sampling import RandomOverSampler
oversampler = RandomOverSampler(random_state=42)

X_resampled, y_resampled = oversampler.fit_resample(X, y)
print(y_resampled.value_counts())

# Split the oversampled data
X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)

#XGBoost classifier
modelxgb = xgb.XGBClassifier(random_state=42)
#train the model
modelxgb.fit(X_train, y_train)
# Make predictions on the testing set
y_predxgb = modelxgb.predict(X_test)

# Evaluate the model
accuracyxgb = accuracy_score(y_test, y_predxgb)
print("Accuracy:",accuracyxgb)
cmxgb = confusion_matrix(y_test, y_predxgb)
print("Confusion Matrix:\n", cmxgb)
reportxgb = classification_report(y_test, y_predxgb)
print(reportxgb)

#ROC Curve for cpomparison
import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, roc_auc_score
tpr, fpr, thresholds = roc_curve(y_test, y_predxgb) #calculate the ROC Curve

# Calculate AUC (Area Under the Curve)
roc_auc = roc_auc_score(y_test, y_predxgb)

# Plot ROC curve
plt.figure(figsize=(8, 6))
plt.plot(tpr, fpr, color='skyblue', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='orange', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve - XGBoost')
plt.legend(loc="lower right")
plt.show()

#Gaussian Naive Bayes classifier
modelnb = GaussianNB()
# Train the model
modelnb.fit(X_train, y_train)
# Make predictions on the testing set
y_prednb = modelnb.predict(X_test)

# Evaluate the model
accuracynb = accuracy_score(y_test, y_prednb)
print("Accuracy:", accuracynb)
cmnb = confusion_matrix(y_test, y_prednb)
print("Confusion Matrix:\n", cmnb)
reportnb = classification_report(y_test, y_prednb)
print(reportnb)

#SVM

from sklearn.svm import SVC
modelsvm = SVC(kernel='linear')
# Train the model
modelsvm.fit(X_train, y_train)
# Make predictions on the testing set
y_predsvm = modelsvm.predict(X_test)

# Evaluate the model
accuracysvm = accuracy_score(y_test, y_predsvm)
print("Accuracy:", accuracysvm)
cmsvm = confusion_matrix(y_test, y_predsvm)
print("Confusion Matrix:\n", cmsvm)
reportsvm = classification_report(y_test, y_predsvm)
print(reportsvm)

#k Nearest Neighbour
import matplotlib.pyplot as plt
import numpy as np
from sklearn.model_selection import cross_val_score
from sklearn.neighbors import KNeighborsClassifier #Import KNeighborsClassifier

param_grid = {'n_neighbors': range(1, 21)} #Define param_grid

k_values = param_grid['n_neighbors']
mean_scores = []

for k in k_values:
    knn = KNeighborsClassifier(n_neighbors=k)
    scores = cross_val_score(knn, X_train, y_train, cv=5, scoring='accuracy')
    mean_scores.append(np.mean(scores))

best_k = k_values[np.argmax(mean_scores)]
print(f"Best k: {best_k}")

# Plotting
plt.plot(k_values, mean_scores)
plt.xlabel('Number of Neighbors (k)')
plt.ylabel('Mean Cross-Validation Accuracy')
plt.title('K-NN Performance for Different k Values')
plt.xticks(k_values)  # Show all k values on the x-axis
plt.show()

from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier(n_neighbors=3) 3the best k value is 1 but we can't take 1 as it would overfit the model

# Train the model
knn.fit(X_train, y_train)
# Make predictions on the testing set
y_predknn = knn.predict(X_test)

# Evaluate the model
accuracyknn = accuracy_score(y_test, y_predknn)
print("Accuracy:", accuracyknn)
cmknn = confusion_matrix(y_test, y_predknn)
print("Confusion Matrix:\n", cmknn)
reportknn = classification_report(y_test, y_predknn)
print(reportknn)

#Logistic Regression model
modellg = LogisticRegression()
# Train the model 
modellg.fit(X_train, y_train)
# Make predictions on the testing data
y_predlg = modellg.predict(X_test)

# Evaluate the model's performance
accuracylg = accuracy_score(y_test, y_predlg)
print("Accuracy:", accuracylg)
cmlg = confusion_matrix(y_test, y_predlg)
print("Confusion Matrix:\n", cmlg)
reportlg = classification_report(y_test, y_predlg)
print(reportlg)

#Decision Tree Classifier
from sklearn.tree import DecisionTreeClassifier
dtmodel = DecisionTreeClassifier(max_depth=5)
# Train the model 
dtmodel.fit(X_train, y_train)
# Make predictions on the testing data
y_preddt = dtmodel.predict(X_test)

# Evaluate the model's performance
accuracydt = accuracy_score(y_test, y_preddt)
print("Accuracy:", accuracydt)
cmdt = confusion_matrix(y_test, y_preddt)
print("Confusion Matrix:\n", cmdt)
reportdt = classification_report(y_test, y_preddt)
print(reportdt)

#Random Forest
from sklearn.ensemble import RandomForestClassifier
rf = RandomForestClassifier(n_estimators=100, random_state=42)
# Train the model 
rf.fit(X_train, y_train)
# Make predictions on the testing data
y_predrf = rf.predict(X_test)

# Evaluate the model's performance
accuracyrf = accuracy_score(y_test, y_predrf)
print("Accuracy:", accuracyrf)
cmrf = confusion_matrix(y_test, y_predrf)
print("Confusion Matrix:\n", cmrf)
reportrf = classification_report(y_test, y_predrf)
print(reportrf)

#plot feature importance
import pandas as pd
import matplotlib.pyplot as plt

importance = rf.feature_importances_ #get feature importance
feature_importance_df = pd.DataFrame({'Feature': X.columns, 'Importance': importance})
feature_importance_df = feature_importance_df.sort_values(by='Importance')
print(feature_importance_df)

# visualize feature importance
plt.figure(figsize=(10, 6))
plt.barh(feature_importance_df['Feature'], feature_importance_df['Importance'],color='skyblue') 
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.title('Feature Importance in Random Forest')
plt.show()

# Calculate ROC curve for comparison
tpr, fpr, thresholds = roc_curve(y_test, y_predrf)
roc_auc = roc_auc_score(y_test, y_predrf)
# Plot ROC curve
plt.figure(figsize=(8, 6))
plt.plot(tpr, fpr, color='skyblue', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='orange', lw=2, linestyle='--')  
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve - Random Forest')
plt.legend(loc="lower right")
plt.show()

#Lift Chart 
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

#function to calculate and plot lift chart
def plot_lift_chart(y_true, y_pred_prob, model_name):
    # Reshape y_pred_prob if it's 2-dimensional
    if y_pred_prob.ndim == 2 and y_pred_prob.shape[1] == 1:
        y_pred_prob = y_pred_prob.flatten()  # Convert to 1D array

    # Create a DataFrame to store true labels and predicted probabilities
    df = pd.DataFrame({'true': y_true, 'pred_prob': y_pred_prob})

    # Sort the DataFrame 
    df = df.sort_values(by=['pred_prob'], ascending=False)

    # Calculate cumulative true positive rate (TPR) and cumulative population
    total_positives = df['true'].sum()
    cumulative_positives = df['true'].cumsum()
    cumulative_population = np.arange(1, len(df) + 1)

    # Calculate lift
    lift = (cumulative_positives / cumulative_population) / (total_positives / len(df))

    # Plot the lift chart
    plt.plot(cumulative_population / len(df), lift, label=f'{model_name}')

# Create a figure and axes
fig, ax = plt.subplots(figsize=(10, 6))

# Plot lift charts for all models
plot_lift_chart(y_test, y_predxgb, 'XGBoost')
plot_lift_chart(y_test, y_prednb, 'Naive Bayes')
plot_lift_chart(y_test, y_predsvm, 'SVM')
plot_lift_chart(y_test, y_predknn, 'K-NN')
plot_lift_chart(y_test, y_predlg, 'Logistic Regression')
plot_lift_chart(y_test, y_preddt, 'Decision Tree')
plot_lift_chart(y_test, y_predrf, 'Random Forest')
plot_lift_chart(y_test, y_predann, 'ANN')
ax.set_xlabel('Cumulative Population (%)')
ax.set_ylabel('Lift')
ax.set_title('Lift Charts for All Models')
ax.legend(loc='upper right')
ax.grid(True)
plt.show()

#Underdsampling
undersampler = RandomUnderSampler(random_state=42)
X_resampled, y_resampled = undersampler.fit_resample(X, y)
print(y_resampled.value_counts())
#split the data 
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)

#XGBoost
import xgboost as xgb
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
modelxgb = xgb.XGBClassifier()
# Train the model
modelxgb.fit(X_train, y_train)
# Make predictions on the testing set
y_predxgb = modelxgb.predict(X_test)

# Evaluate the model
accuracyxgb = accuracy_score(y_test, y_predxgb)
print("Accuracy:", accuracyxgb)
cmxgb = confusion_matrix(y_test, y_predxgb)
print("Confusion Matrix:\n", cmxgb)
reportxgb = classification_report(y_test, y_predxgb)
print(reportxgb)

#k Nearest Neighbours
import matplotlib.pyplot as plt
import numpy as np
from sklearn.model_selection import cross_val_score
from sklearn.neighbors import KNeighborsClassifier

param_grid = {'n_neighbors': range(1, 21)} 

k_values = param_grid['n_neighbors']
mean_scores = []

for k in k_values:
    knn = KNeighborsClassifier(n_neighbors=k)
    scores = cross_val_score(knn, X_train, y_train, cv=5, scoring='accuracy')
    mean_scores.append(np.mean(scores))

best_k = k_values[np.argmax(mean_scores)]
print(f"Best k: {best_k}")

# Plotting
plt.plot(k_values, mean_scores)
plt.xlabel('Number of Neighbors (k)')
plt.ylabel('Mean Cross-Validation Accuracy')
plt.title('K-NN Performance for Different k Values')
plt.xticks(k_values)  # Show all k values on the x-axis
plt.show()

rom sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

knn = KNeighborsClassifier(n_neighbors=best_k)
# Train the model
knn.fit(X_train, y_train)
# Make predictions on the testing set
y_pred_knn = knn.predict(X_test)

# Evaluate the model
accuracy_knn = accuracy_score(y_test, y_pred_knn)
print("Accuracy:", accuracy_knn)
cm_knn = confusion_matrix(y_test, y_pred_knn)
print("Confusion Matrix:\n", cm_knn)
report_knn = classification_report(y_test, y_pred_knn)
print(report_knn)

# SVM  
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

svm = SVC(kernel='linear', random_state=42)
# Train the model
svm.fit(X_train, y_train)
# Make predictions on the testing set
y_pred_svm = svm.predict(X_test)

# Evaluate the model
accuracy_svm = accuracy_score(y_test, y_pred_svm)
print("Accuracy:", accuracy_svm)
cm_svm = confusion_matrix(y_test, y_pred_svm)
print("Confusion Matrix:\n", cm_svm)
report_svm = classification_report(y_test, y_pred_svm)
print(report_svm)

# Gaussian Naive Bayes classifier
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

nb_model = GaussianNB()
# Train the model
nb_model.fit(X_train, y_train)
# Make predictions on the testing set
y_pred_nb = nb_model.predict(X_test)

# Evaluate the model
accuracy_nb = accuracy_score(y_test, y_pred_nb)
print("Accuracy:", accuracy_nb)
cm_nb = confusion_matrix(y_test, y_pred_nb)
print("Confusion Matrix:\n", cm_nb)
report_nb = classification_report(y_test, y_pred_nb)
print(report_nb)

#Logistic Regression model
modellg = LogisticRegression()
# Train the model
modellg.fit(X_train, y_train)
# Make predictions on the testing data
y_predlg = modellg.predict(X_test)

# Evaluate the model
accuracylg = accuracy_score(y_test, y_predlg)
print("Accuracy:", accuracylg)
cmlg = confusion_matrix(y_test, y_predlg)
print("Confusion Matrix:\n", cmlg)
reportlg = classification_report(y_test, y_predlg)
print(reportlg)

#Decision Tree Classifier
from sklearn.tree import DecisionTreeClassifier
dtmodel = DecisionTreeClassifier(max_depth=5)
# Train the model 
dtmodel.fit(X_train, y_train)
# Make predictions on the testing data
y_preddt = dtmodel.predict(X_test)

# Evaluate the model
accuracydt = accuracy_score(y_test, y_preddt)
print("Accuracy:", accuracydt)
cmdt = confusion_matrix(y_test, y_preddt)
print("Confusion Matrix:\n", cmdt)
reportdt = classification_report(y_test, y_preddt)
print(reportdt)

#Random Forest

from sklearn.ensemble import RandomForestClassifier
rf = RandomForestClassifier(n_estimators=100, random_state=42)
# Train the model 
rf.fit(X_train, y_train)
# Make predictions on the testing data
y_predrf = rf.predict(X_test)

# Evaluate the model
accuracyrf = accuracy_score(y_test, y_predrf)
print("Accuracy:", accuracyrf)
cmrf = confusion_matrix(y_test, y_predrf)
print("Confusion Matrix:\n", cmrf)
reportrf = classification_report(y_test, y_predrf)
print(reportrf)

#ANN
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
import pandas as pd

#use one hot encoding 
X_train = pd.get_dummies(X_train, columns=X_train.select_dtypes(include=['object', 'category']).columns)
X_test = pd.get_dummies(X_test, columns=X_test.select_dtypes(include=['object', 'category']).columns)

for col in X_train.select_dtypes(include=['category']).columns:
    X_train[col] = X_train[col].cat.codes  # Convert to numerical codes

for col in X_test.select_dtypes(include=['category']).columns:
    X_test[col] = X_test[col].cat.codes  # Convert to numerical codes

if y_train.dtype in ['object', 'category']:
    y_train = pd.factorize(y_train)[0]  # Convert to numerical using factorize

if y_test.dtype in ['object', 'category']:
    y_test = pd.factorize(y_test)[0]  # Convert to numerical using factorize

# Create a Sequential model
modelseq = Sequential()

# Add layers to the model
modelseq.add(Dense(12,input_dim=X_train.shape[1], activation='relu'))  # Input layer
modelseq.add(Dense(8, activation='relu'))  # Hidden layer
modelseq.add(Dense(1, activation='sigmoid'))  # Output layer

# Compile the model
modelseq.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
# Train the model
modelseq.fit(X_train, y_train, epochs=10, batch_size=32)
# Make predictions on the testing data
y_predseq = modelseq.predict(X_test)
y_predseq = (y_predseq > 0.5)  # Convert probabilities to binary predictions

# Evaluate the model
accuracyseq = accuracy_score(y_test, y_predseq)
print("Accuracy:", accuracyseq)
cmseq = confusion_matrix(y_test, y_predseq)
print("Confusion Matrix:\n", cmseq)
reportseq = classification_report(y_test, y_predseq)
print(reportseq)

#Combination-SMOTE&ENN

!pip install imbalanced-learn

import pandas as pd
from imblearn.combine import SMOTEENN
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

X = data.drop('Conversion', axis=1)
y = data['Conversion']

#combine oversampling and undersampling
smote_enn = SMOTEENN(random_state=42)
X_resampled, y_resampled = smote_enn.fit_resample(X, y)
print(y_resampled.value_counts())
#split the data
X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)

#XGBoost 
import xgboost as xgb
xgb_model = xgb.XGBClassifier()
# Train the model
xgb_model.fit(X_train, y_train)
# Make predictions
y_pred_xgb = xgb_model.predict(X_test)

# Evaluate the model
accuracy_xgb = accuracy_score(y_test, y_pred_xgb)
print("Accuracy:", accuracy_xgb)
cm_xgb = confusion_matrix(y_test, y_pred_xgb)
print("Confusion Matri:\n", cm_xgb)
report_xgb = classification_report(y_test, y_pred_xgb)
print("Classification Report (XGBoost):\n", report_xgb)

#k Nearest Neighnours
import matplotlib.pyplot as plt
import numpy as np
from sklearn.model_selection import cross_val_score
from sklearn.neighbors import KNeighborsClassifier #Import KNeighborsClassifier

param_grid = {'n_neighbors': range(1, 21)} #Define param_grid

k_values = param_grid['n_neighbors']
mean_scores = []

for k in k_values:
    knn = KNeighborsClassifier(n_neighbors=k)
    scores = cross_val_score(knn, X_train, y_train, cv=5, scoring='accuracy')
    mean_scores.append(np.mean(scores))
# Plotting
plt.plot(k_values, mean_scores)
plt.xlabel('Number of Neighbors (k)')
plt.ylabel('Mean Cross-Validation Accuracy')
plt.title('K-NN Performance for Different k Values')
plt.xticks(k_values)  # Show all k values on the x-axis
plt.show()

from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import GridSearchCV
knn = KNeighborsClassifier(n_neighbors=4)
#train the model
knn.fit(X_train, y_train)

#Predictions and Evaluation
y_predknn = knn.predict(X_test)
accuracyknn = accuracy_score(y_test, y_predknn)
cmknn = confusion_matrix(y_test, y_predknn)
reportknn = classification_report(y_test, y_predknn)
print(f"Accuracy: {accuracyknn}")
print(f"Confusion Matrix:\n{cmknn}")
print(f"Classification Report(k-nn):\n{reportknn}")

#SVM
from sklearn.svm import SVC
# Create an SVM classifier
modelsvm = SVC(kernel='linear')
# Train the model
modelsvm.fit(X_train, y_train)
# Make predictions on the testing set
y_predsvm = modelsvm.predict(X_test)

# Evaluate the model
accuracysvm = accuracy_score(y_test, y_predsvm)
print("Accuracy:", accuracysvm)
reportsvm = classification_report(y_test, y_predsvm)
print(f"Classification Report(svm):\n{reportsvm}")
# Print the confusion matrix
cmsvm = confusion_matrix(y_test, y_predsvm)
print("Confusion Matrix:\n", cmsvm)



#Gaussian Naive Bayes
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

nb_model = GaussianNB()
# Train the model
nb_model.fit(X_train, y_train)
# Make predictions
y_pred_nb = nb_model.predict(X_test)

# Evaluate the model
accuracy_nb = accuracy_score(y_test, y_pred_nb)
print("Accuracy (Naive Bayes):", accuracy_nb)
cm_nb = confusion_matrix(y_test, y_pred_nb)
print("Confusion Matrix (Naive Bayes):\n", cm_nb)
report_nb = classification_report(y_test, y_pred_nb)
print("Classification Report (Naive Bayes):\n", report_nb)

#Logistic Regression model
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
logreg_model = LogisticRegression()
# Train the model
logreg_model.fit(X_train, y_train)
# Make predictions
y_pred_logreg = logreg_model.predict(X_test)

# Evaluate the model
accuracy_logreg = accuracy_score(y_test, y_pred_logreg)
print("Accuracy (Logistic Regression):", accuracy_logreg)
cm_logreg = confusion_matrix(y_test, y_pred_logreg)
print("Confusion Matrix (Logistic Regression):\n", cm_logreg)
report_logreg = classification_report(y_test, y_pred_logreg)
print("Classification Report (Logistic Regression):\n", report_logreg)

#Decision Tree Model
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import roc_curve, auc, accuracy_score, classification_report, confusion_matrix
  
dt_model = DecisionTreeClassifier()
# Train the model
dt_model.fit(X_train, y_train)
# Make predictions
y_pred_dt = dt_model.predict(X_test)

# Evaluate the model
accuracy_dt = accuracy_score(y_test, y_pred_dt)
print("Accuracy (Decision Tree):", accuracy_dt)
cm_dt = confusion_matrix(y_test, y_pred_dt)
print("Confusion Matrix (Decision Tree):\n", cm_dt)
report_dt = classification_report(y_test, y_pred_dt)
print("Classification Report (Decision Tree):\n", report_dt)

#Random Forest 
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)  
# Train the model
rf_model.fit(X_train, y_train)
# Make predictions
y_pred_rf = rf_model.predict(X_test)

# Evaluate the model
accuracy_rf = accuracy_score(y_test, y_pred_rf)
print("Accuracy (Random Forest):", accuracy_rf)
cm_rf = confusion_matrix(y_test, y_pred_rf)
print("Confusion Matrix (Random Forest):\n", cm_rf)
report_rf = classification_report(y_test, y_pred_rf)
print("Classification Report (Random Forest):\n", report_rf)

#ANN
import matplotlib.pyplot as plt
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import roc_curve, auc, accuracy_score, classification_report, confusion_matrix

ann_model = MLPClassifier(hidden_layer_sizes=(100,),  # Adjust hidden layer sizes
                          activation='relu',  # Activation function
                          solver='adam',  # Optimization algorithm
                          max_iter=200,  # Maximum iterations
                          random_state=42)  # For reproducibility
# Train the model
ann_model.fit(X_train, y_train)
# Make predictions
y_pred_ann = ann_model.predict(X_test)

# Evaluate the model
accuracy_ann = accuracy_score(y_test, y_pred_ann)
print("Accuracy (ANN):", accuracy_ann)
cm_ann = confusion_matrix(y_test, y_pred_ann)
print("Confusion Matrix (ANN):\n", cm_ann)
report_ann = classification_report(y_test, y_pred_ann)
print("Classification Report (ANN):\n", report_ann)

#Original Data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

#XGBoost classifier
import xgboost as xgb
modelxgb = xgb.XGBClassifier(random_state=42)
modelxgb.fit(X_train, y_train)
# Make predictions on the testing set
y_predxgb = modelxgb.predict(X_test)

# Evaluate the model
accuracyxgb = accuracy_score(y_test, y_predxgb)
print("Accuracy:",accuracyxgb)
cmxgb = confusion_matrix(y_test, y_predxgb)
print("Confusion Matrix:\n", cmxgb)
report_xgb = classification_report(y_test, y_predxgb)
print("Classification Report (XGB):\n", report_xgb)

#Logistic Regression model
modellg = LogisticRegression()
# Train the model
modellg.fit(X_train, y_train)
# Make predictions on the testing data
y_predlg = modellg.predict(X_test)

# Evaluate the model's performance
accuracylg = accuracy_score(y_test, y_predlg)
print("Accuracy:", accuracylg)
cmlg = confusion_matrix(y_test, y_predlg)
print("Confusion Matrix:\n", cmlg)
report_lg = classification_report(y_test, y_predlg)
print("Classification Report (LR):\n", report_lg)

#k Nearest Neighbors
import matplotlib.pyplot as plt
import numpy as np
from sklearn.model_selection import cross_val_score
from sklearn.neighbors import KNeighborsClassifier 

param_grid = {'n_neighbors': range(1, 21)} 

k_values = param_grid['n_neighbors']
mean_scores = []

for k in k_values:
    knn = KNeighborsClassifier(n_neighbors=k)
    scores = cross_val_score(knn, X_train, y_train, cv=5, scoring='accuracy')
    mean_scores.append(np.mean(scores))

best_k = k_values[np.argmax(mean_scores)]
print(f"Best k: {best_k}")

# Plotting
plt.plot(k_values, mean_scores)
plt.xlabel('Number of Neighbors (k)')
plt.ylabel('Mean Cross-Validation Accuracy')
plt.title('K-NN Performance for Different k Values')
plt.xticks(k_values)
plt.show()

knn = KNeighborsClassifier(n_neighbors=best_k)
#Train the model
knn.fit(X_train, y_train)
#Predictions and Evaluation
y_predknn = knn.predict(X_test)

#Evaluate the model
accuracyknn = accuracy_score(y_test, y_predknn)
cmknn = confusion_matrix(y_test, y_predknn)
reportknn = classification_report(y_test, y_predknn)
print(f"Accuracy: {accuracyknn}")
print(f"Confusion Matrix:\n{cmknn}")
print(f"Classification Report(k-nn):\n{reportknn}")

#SVM
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

svm = SVC(kernel='linear', random_state=42)
# Train the model
svm.fit(X_train, y_train)
# Make predictions on the testing set
y_pred_svm = svm.predict(X_test)

# Evaluate the model
accuracy_svm = accuracy_score(y_test, y_pred_svm)
print("Accuracy:", accuracy_svm)
cm_svm = confusion_matrix(y_test, y_pred_svm)
print("Confusion Matrix:\n", cm_svm)
report_svm = classification_report(y_test, y_pred_svm)
print("Classification Report (SVM):\n", report_svm)

#Gaussian Naive Bayes classifier
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

nb_model = GaussianNB()
# Train the model
nb_model.fit(X_train, y_train)
# Make predictions on the testing set
y_pred_nb = nb_model.predict(X_test)

# Evaluate the model
accuracy_nb = accuracy_score(y_test, y_pred_nb)
print("Accuracy:", accuracy_nb)
cm_nb = confusion_matrix(y_test, y_pred_nb)
print("Confusion Matrix:\n", cm_nb)
report_nb = classification_report(y_test, y_pred_nb)
print(report_nb)

#Decision Tree Classifier
from sklearn.tree import DecisionTreeClassifier
dtmodel = DecisionTreeClassifier(max_depth=5)
# Train the model 
dtmodel.fit(X_train, y_train)
# Make predictions on the testing data
y_preddt = dtmodel.predict(X_test)

# Evaluate the model
accuracydt = accuracy_score(y_test, y_preddt)
print("Accuracy:", accuracydt)
cmdt = confusion_matrix(y_test, y_preddt)
print("Confusion Matrix:\n", cmdt)
report_dt = classification_report(y_test, y_preddt)
print(report_dt)

#Random Forest
from sklearn.ensemble import RandomForestClassifier
rf = RandomForestClassifier(n_estimators=100, random_state=42)
# Train the model
rf.fit(X_train, y_train)
# Make predictions on the testing data
y_predrf = rf.predict(X_test)

# Evaluate the model
accuracyrf = accuracy_score(y_test, y_predrf)
print("Accuracy:", accuracyrf)
cmrf = confusion_matrix(y_test, y_predrf)
print("Confusion Matrix:\n", cmrf)
reportrf = classification_report(y_test, y_predrf)
print(reportrf)

#ANN
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
import pandas as pd


#Use one-hot encoding
X_train = pd.get_dummies(X_train, columns=X_train.select_dtypes(include=['object', 'category']).columns)
X_test = pd.get_dummies(X_test, columns=X_test.select_dtypes(include=['object', 'category']).columns)

for col in X_train.select_dtypes(include=['category']).columns:
    X_train[col] = X_train[col].cat.codes  # Convert to numerical codes

for col in X_test.select_dtypes(include=['category']).columns:
    X_test[col] = X_test[col].cat.codes  # Convert to numerical codes

if y_train.dtype in ['object', 'category']:
    y_train = pd.factorize(y_train)[0]  # Convert to numerical using factorize

if y_test.dtype in ['object', 'category']:
    y_test = pd.factorize(y_test)[0]  # Convert to numerical using factorize

# Create a Sequential model
modelseq = Sequential()

# Add layers to the model
modelseq.add(Dense(12,input_dim=X_train.shape[1], activation='relu'))  # Input layer
modelseq.add(Dense(8, activation='relu'))  # Hidden layer
modelseq.add(Dense(1, activation='sigmoid'))  # Output layer

# Compile the model
modelseq.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
# Train the model
modelseq.fit(X_train, y_train, epochs=10, batch_size=32)
# Make predictions on the testing data
y_predseq = modelseq.predict(X_test)
y_predseq = (y_predseq > 0.5)  # Convert probabilities to binary predictions

# Evaluate the model
accuracyseq = accuracy_score(y_test, y_predseq)
print("Accuracy:", accuracyseq)
cmseq = confusion_matrix(y_test, y_predseq)
print("Confusion Matrix:\n", cmseq)
reportseq = classification_report(y_test, y_predseq)
print(reportseq)
